{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_HW2_P1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "source": [
        "dataset = pd.read_csv('Agaricus-lepiota.data.txt', header=None)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsz2yCebe1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e81dc71-ca73-4c14-89a3-5cb102cccb33"
      },
      "source": [
        "print(dataset.loc[dataset[11] == \"?\"])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22\n",
            "3984  e  x  y  b  t  n  f  c  b  e  e  ?  s  s  e  w  p  w  t  e  w  c  w\n",
            "4023  p  x  y  e  f  y  f  c  n  b  t  ?  k  s  w  w  p  w  o  e  w  v  p\n",
            "4076  e  f  y  u  f  n  f  c  n  h  e  ?  s  f  w  w  p  w  o  f  h  y  d\n",
            "4100  p  x  y  e  f  y  f  c  n  b  t  ?  k  s  p  p  p  w  o  e  w  v  d\n",
            "4104  p  x  y  n  f  f  f  c  n  b  t  ?  s  s  p  p  p  w  o  e  w  v  l\n",
            "...  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
            "8119  e  k  s  n  f  n  a  c  b  y  e  ?  s  s  o  o  p  o  o  p  b  c  l\n",
            "8120  e  x  s  n  f  n  a  c  b  y  e  ?  s  s  o  o  p  n  o  p  b  v  l\n",
            "8121  e  f  s  n  f  n  a  c  b  n  e  ?  s  s  o  o  p  o  o  p  b  c  l\n",
            "8122  p  k  y  n  f  y  f  c  n  b  t  ?  s  k  w  w  p  w  o  e  w  v  l\n",
            "8123  e  x  s  n  f  n  a  c  b  y  e  ?  s  s  o  o  p  o  o  p  o  c  l\n",
            "\n",
            "[2480 rows x 23 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93k7ipkSexq"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=\"?\", strategy='most_frequent')\n",
        "dataset = pd.DataFrame(imputer.fit_transform(dataset))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UgLdMS_bjq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d198ea-00fd-4623-8313-dcbbec27d585"
      },
      "source": [
        "print(dataset.loc[dataset[11] == \"?\"])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXgA6CzlqbCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a6c37c-5007-4e2c-a834-9e1650c01847"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "train_data_index , test_data_index = next(kf.split(dataset), None)\n",
        "train_data = dataset.iloc[train_data_index]\n",
        "test_data = dataset.iloc[test_data_index]\n",
        "\n",
        "print(\"\\t\\t\\tTrain data \\n\" ,train_data ,\"\\n\")\n",
        "print(\"\\t\\t\\tTest data \\n\" ,test_data)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tTrain data \n",
            "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22\n",
            "0     p  x  s  n  t  p  f  c  n  k  e  e  s  s  w  w  p  w  o  p  k  s  u\n",
            "1     e  x  s  y  t  a  f  c  b  k  e  c  s  s  w  w  p  w  o  p  n  n  g\n",
            "2     e  b  s  w  t  l  f  c  b  n  e  c  s  s  w  w  p  w  o  p  n  n  m\n",
            "3     p  x  y  w  t  p  f  c  n  n  e  e  s  s  w  w  p  w  o  p  k  s  u\n",
            "4     e  x  s  g  f  n  f  w  b  k  t  e  s  s  w  w  p  w  o  e  n  a  g\n",
            "...  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
            "8118  p  k  y  n  f  f  f  c  n  b  t  b  k  s  p  w  p  w  o  e  w  v  d\n",
            "8119  e  k  s  n  f  n  a  c  b  y  e  b  s  s  o  o  p  o  o  p  b  c  l\n",
            "8121  e  f  s  n  f  n  a  c  b  n  e  b  s  s  o  o  p  o  o  p  b  c  l\n",
            "8122  p  k  y  n  f  y  f  c  n  b  t  b  s  k  w  w  p  w  o  e  w  v  l\n",
            "8123  e  x  s  n  f  n  a  c  b  y  e  b  s  s  o  o  p  o  o  p  o  c  l\n",
            "\n",
            "[7311 rows x 23 columns] \n",
            "\n",
            "\t\t\tTest data \n",
            "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22\n",
            "21    p  x  y  n  t  p  f  c  n  n  e  e  s  s  w  w  p  w  o  p  n  v  g\n",
            "34    e  b  y  y  t  l  f  c  b  n  e  c  s  s  w  w  p  w  o  p  n  s  m\n",
            "44    e  x  s  y  t  a  f  c  b  w  e  c  s  s  w  w  p  w  o  p  k  n  m\n",
            "59    e  x  y  n  t  a  f  c  b  p  e  r  s  y  w  w  p  w  o  p  k  y  p\n",
            "84    e  x  y  y  t  l  f  c  b  w  e  r  s  y  w  w  p  w  o  p  k  s  g\n",
            "...  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
            "8064  p  k  y  e  f  f  f  c  n  b  t  b  s  s  p  p  p  w  o  e  w  v  l\n",
            "8071  e  k  s  n  f  n  a  c  b  n  e  b  s  s  o  o  p  o  o  p  y  v  l\n",
            "8089  p  k  y  e  f  f  f  c  n  b  t  b  k  k  w  p  p  w  o  e  w  v  d\n",
            "8110  e  x  s  n  f  n  a  c  b  o  e  b  s  s  o  o  p  o  o  p  n  v  l\n",
            "8120  e  x  s  n  f  n  a  c  b  y  e  b  s  s  o  o  p  n  o  p  b  v  l\n",
            "\n",
            "[813 rows x 23 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxjSUXFQqo-3"
      },
      "source": [
        "def entropy(target_col):\n",
        "\n",
        "    #Calculate the entropy of a dataset.\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWe_-UoZJei_"
      },
      "source": [
        "elements,counts = np.unique(dataset[1],return_counts = True)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvmiL9N5KDk0",
        "outputId": "82117d51-6c93-4a45-9ebb-ad94fca5e144"
      },
      "source": [
        "print(elements, counts, np.sum(counts))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['b' 'c' 'f' 'k' 's' 'x'] [ 452    4 3152  828   32 3656] 8124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86LdpTZKIaNV"
      },
      "source": [
        "def InfoGain(data, split_attribute_index, label_index = 0):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of a dataset. This function takes three parameters:\n",
        "    1. data = The dataset for whose feature the IG should be calculated\n",
        "    2. split_attribute_index = the index of the feature for which the information gain should be calculated\n",
        "    3. label_index = label_index of the target feature.\n",
        "    \"\"\"    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[label_index])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_index],return_counts=True)\n",
        "\n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_index]==vals[i]).dropna()[label_index]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-BX6ySFQ-ad",
        "outputId": "2e470518-db2c-44e8-d3ec-b6ca9609b9b5"
      },
      "source": [
        "print(InfoGain(dataset, 5))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9060749773839998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1rg-vxoIdre"
      },
      "source": [
        "def ID3(data, originaldata, features, label_index = 0, parent_node_class = None):\n",
        "    \"\"\"\n",
        "    ID3 Algorithm: This function takes five paramters:\n",
        "    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset\n",
        " \n",
        "    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset\n",
        "    in the case the dataset delivered by the first parameter is empty\n",
        "\n",
        "    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process\n",
        "    we have to remove features from our dataset --> Splitting at each node\n",
        "\n",
        "    4. label_index = the index of the target attribute\n",
        "\n",
        "    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is \n",
        "    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature\n",
        "    space, we want to return the mode target feature value of the direct parent node.\n",
        "    \"\"\"   \n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[label_index])) <= 1:\n",
        "        return np.unique(data[label_index])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the max value feature in the original dataset\n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[label_index])[np.argmax(np.unique(originaldata[label_index],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    \n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[label_index])[np.argmax(np.unique(data[label_index],return_counts=True)[1])]\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,label_index) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,originaldata,features,label_index,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQdYRcu60vhs"
      },
      "source": [
        "def predict(query,tree,default = \"p\"):\n",
        "    \"\"\"\n",
        "    Prediction of a new/unseen query instance. This takes two parameters:\n",
        "    1. The query instance as a dictionary of the shape {\"feature_name\":feature_value,...}\n",
        "\n",
        "    2. The tree \n",
        "    SUMMARIZED: If we have a query instance consisting of values for features, we take this features and check if the \n",
        "    name of the root node is equal to one of the query features.\n",
        "    If this is true, we run down the root node outgoing branch whose value equals the value of query feature == the root node.\n",
        "    If we find at the end of this branch a leaf node (not a dict object) we return this value (this is our prediction).\n",
        "    If we instead find another node (== sub_tree == dict objct) we search in our query for the feature which equals the value \n",
        "    of that node. Next we look up the value of our query feature and run down the branch whose value is equal to the \n",
        "    query[key] == query feature value. And as you can see this is exactly the recursion we talked about\n",
        "    with the important fact that for each node we run down the tree, we check only the nodes and branches which are \n",
        "    below this node and do not run the whole tree beginning at the root node \n",
        "    --> This is why we re-call the classification function with 'result'\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    #1.\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            #2.\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            #3.\n",
        "            result = tree[key][query[key]]\n",
        "            #4.\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9sTvJh-0zMk"
      },
      "source": [
        "def test(data,tree):\n",
        "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
        "    #convert it to a dictionary\n",
        "    queries = data.iloc[:,1:].to_dict(orient = \"records\")\n",
        "\n",
        "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    #Calculate the prediction accuracy\n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,\"p\") \n",
        "    return (np.sum(predicted[\"predicted\"] == data[0].values )/len(data))*100"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ37Y5KQyVG2"
      },
      "source": [
        "# Running the Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JfnrmqMyfX2",
        "outputId": "1d32b495-2d10-429f-cb61-956f60389cbf"
      },
      "source": [
        "accuracy = []\n",
        "for i in range(10):\n",
        "  train_data_index , test_data_index = next(kf.split(dataset), None)\n",
        "  train_data = dataset.iloc[train_data_index].reset_index(drop=True)\n",
        "  test_data = dataset.iloc[test_data_index].reset_index(drop=True)\n",
        "  features = train_data.columns[1:]\n",
        "  tree = ID3(train_data, train_data, features )\n",
        "  accuracy.append(test(test_data,tree))\n",
        "  \n",
        "pprint(tree)\n",
        "print(\"\\nAverage accuracy of 10 runs of program\" , u\"\\u00B1 STD = %.2f%%\" % np.mean(accuracy), u\"\\u00B1 %.2f\" % np.std(accuracy))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{5: {'a': 'e',\n",
            "     'c': 'p',\n",
            "     'f': 'p',\n",
            "     'l': 'e',\n",
            "     'm': 'p',\n",
            "     'n': {20: {'b': 'e',\n",
            "                'h': 'e',\n",
            "                'k': 'e',\n",
            "                'n': 'e',\n",
            "                'o': 'e',\n",
            "                'r': 'p',\n",
            "                'w': {22: {'d': {8: {'b': 'e', 'n': 'p'}},\n",
            "                           'g': 'e',\n",
            "                           'l': {3: {'c': 'e', 'n': 'e', 'w': 'p', 'y': 'p'}},\n",
            "                           'p': 'e',\n",
            "                           'w': 'e'}},\n",
            "                'y': 'e'}},\n",
            "     'p': 'p',\n",
            "     's': 'p',\n",
            "     'y': 'p'}}\n",
            "\n",
            "Average accuracy of 10 runs of program ± STD = 100.00% ± 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}