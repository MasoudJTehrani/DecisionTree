# -*- coding: utf-8 -*-
"""ML_HW2_P2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ET4CtzjcZi5VyX49Ew-n2FSvZiLv6GXE

# Data Preprocessing

## Importing the libraries
"""

import numpy as np
import pandas as pd
from pprint import pprint
import matplotlib.pyplot as plt
from matplotlib import style
style.use("fivethirtyeight")

"""## Importing the dataset"""

dataset1 = pd.read_csv('EnjoySport.txt', header=None, sep="\t")
dataset2 = pd.read_csv('Automobile2.txt', header=None, sep="\t")

#df = pd.read_csv('Automobile.txt', header=None, sep='\n')
#df = df[0].str.split('\t', expand=True)
#df.iloc[:, :12].to_csv("Automobile2.txt",sep="\t",header=False,index=False)

print(dataset1, "\n")
print(dataset2)

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dataset1.iloc[:, :-1], dataset1.iloc[:, -1], test_size = 0.3, shuffle=True)

train_data = pd.concat([X_train, y_train], axis =1)
test_data = pd.concat([X_test, y_test], axis =1)

print(train_data, '\n')
print(test_data)

"""## Functions"""

def STD(target_col):
  
    #Calculate the STD of a dataset.
    return np.std(target_col)

print(STD(dataset1.iloc[:,-1]))
print(STD(dataset2.iloc[:,-1]))

print(STD([46,43,52,44]))

def STDR(data, split_attribute_index, label_index = -1):
    """
    Calculate the information gain of a dataset. This function takes three parameters:
    1. data = The dataset for whose feature the IG should be calculated
    2. split_attribute_index = the index of the feature for which the information gain should be calculated
    3. label_index = label_index of the target feature.
    """    
    #Calculate the entropy of the total dataset
    total_STD = STD(data.iloc[:,label_index])
    
    ##Calculate the entropy of the dataset
    
    #Calculate the values and the corresponding counts for the split attribute 
    vals,counts= np.unique(data[split_attribute_index],return_counts=True)

    #Calculate the weighted entropy
    Weighted_STD = np.sum([(counts[i]/np.sum(counts))*STD(data.where(data[split_attribute_index]==vals[i]).dropna().iloc[:,label_index]) for i in range(len(vals))])
    
    #Calculate the information gain
    STDR = total_STD - Weighted_STD
    return STDR

print(STDR(dataset1, 0))
print(STDR(dataset1, 1))
print(STDR(dataset1, 2))
print(STDR(dataset1, 3))
print(STDR(dataset2, 3))

def ID3(data, originaldata, features, min_samples, min_CV, label_index = -1, parent_node_class = None):
    """
    ID3 Algorithm: This function takes five paramters:
    1. data = the data for which the ID3 algorithm should be run --> In the first run this equals the total dataset
 
    2. originaldata = This is the original dataset needed to calculate the mode target feature value of the original dataset
    in the case the dataset delivered by the first parameter is empty

    3. features = the feature space of the dataset . This is needed for the recursive call since during the tree growing process
    we have to remove features from our dataset --> Splitting at each node

    4. label_index = the index of the target attribute

    5. parent_node_class = This is the value or class of the mode target feature value of the parent node for a specific node. This is 
    also needed for the recursive call since if the splitting leads to a situation that there are no more features left in the feature
    space, we want to return the mode target feature value of the direct parent node.
    """   
    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#

    #If the dataset is empty, return the mean target feature value in the original dataset
    if len(data)==0:
        return np.mean(originaldata.iloc[:,label_index])
    
    #If all target_values have the same value, return the mean value of the target feature for this dataset
    elif len(data) <= min_samples or (STD(data.iloc[:,label_index]) / np.mean(data.iloc[:,label_index]) * 100) < min_CV:
        return np.mean(data.iloc[:,label_index])
    #######################################################

    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that
    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence
    #the mode target feature value is stored in the parent_node_class variable.
    
    elif len(features) ==0:
        return parent_node_class
    
    #If none of the above holds true, grow the tree!
    
    else:
        #Set the default value for this node --> The mode target feature value of the current node
        parent_node_class = np.mean(data.iloc[:,label_index])
        
        #Select the feature which best splits the dataset
        item_values = [STDR(data,feature,label_index) for feature in features] #Return the STDR values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        
        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information
        #gain in the first run
        tree = {best_feature:{}}
        
        
        #Remove the feature with the best inforamtion gain from the feature space
        features = [i for i in features if i != best_feature]
        
        #Grow a branch under the root node for each possible value of the root node feature
        
        for value in np.unique(data[best_feature]):
            value = value
            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets
            sub_data = data.where(data[best_feature] == value).dropna()
            
            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!
            subtree = ID3(sub_data,originaldata,features, min_samples, min_CV, label_index,parent_node_class)
            
            #Add the sub tree, grown from the sub_dataset to the tree under the root node
            tree[best_feature][value] = subtree
            
        return(tree)

def predict(query,tree,default):   
    
    #1.
    for key in list(query.keys()):
        if key in list(tree.keys()):
            #2.
            try:
                result = tree[key][query[key]] 
            except:
                return default
  
            #3.
            result = tree[key][query[key]]
            #4.
            if isinstance(result,dict):
                return predict(query,result, default)

            else:
                return result

def test(data,tree, mean_data):
    #Create new query instances by simply removing the target feature column from the original dataset and 
    #convert it to a dictionary
    queries = data.iloc[:, :-1].to_dict(orient = "records")

    #Create a empty DataFrame in whose columns the prediction of the tree are stored
    predicted = []

    #Calculate the prediction accuracy
    for i in range(len(data)):
        predicted.append(predict(queries[i],tree,mean_data)) 
    RMSE = np.sqrt(np.sum(((data.iloc[:,-1]-predicted)**2)/len(data)))
    return RMSE

"""# Running the Code"""

mean_data1 = np.mean(dataset1.iloc[:,-1])
mean_data2 = np.mean(dataset2.iloc[:,-1])

averageRMSE1 = []
averageRMSE2 = []

for i in range(10):
  X_train, X_test, y_train, y_test = train_test_split(dataset1.iloc[:, :-1], dataset1.iloc[:, -1], test_size = 0.3, shuffle=True)
  train_data = pd.concat([X_train, y_train], axis =1)
  test_data = pd.concat([X_test, y_test], axis =1)

  features = train_data.columns[:-1]
  tree = ID3(train_data, train_data, features, 3, 10)
  averageRMSE1.append(test(test_data,tree, mean_data1))

print("\t\t\tEnjoySport\n")
pprint(tree)
print("\nAverage root mean square error (RMSE) for EnjoySport:" , u"\u00B1 STD = %.2f" % np.mean(averageRMSE1), u"\u00B1 %.2f" % STD(averageRMSE1) ,"\n\n")

for i in range(10):
  X_train, X_test, y_train, y_test = train_test_split(dataset2.iloc[:, :-1], dataset2.iloc[:, -1], test_size = 0.3, shuffle=True)
  train_data = pd.concat([X_train, y_train], axis =1)
  test_data = pd.concat([X_test, y_test], axis =1)

  features = train_data.columns[:-1]
  tree = ID3(train_data, train_data, features, 5, 10)
  averageRMSE2.append(test(test_data,tree, mean_data2))

print("-"*100)
print("\t\t\t\tAutomobile\n")
pprint(tree)
print("\nAverage root mean square error (RMSE) for Automobile:" , u"\u00B1 STD = %.2f" % np.mean(averageRMSE2), u"\u00B1 %.2f" % STD(averageRMSE2))

"""# Plotting the result"""

"""
Plot the MSE with respect to the minimum number of instances
""" 
fig = plt.figure()
ax0 = fig.add_subplot(111) 

RMSE_test = []
RMSE_train = []
for i in range(1,50):
    tree = ID3(train_data,train_data,train_data.columns[:-1],i,10)
    RMSE_test.append(test(test_data,tree,mean_data2)) 
    RMSE_train.append(test(train_data,tree,mean_data2))
   
ax0.plot(range(1,50),RMSE_test,label='Test_Data')
ax0.plot(range(1,50),RMSE_train,label='Train_Data')
ax0.legend()
ax0.set_title('RMSE with respect to the minumim number of instances per node')
ax0.set_xlabel('#Instances')
ax0.set_ylabel('RMSE')
plt.show()